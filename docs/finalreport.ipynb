{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning for Exoplanet Observability</h1>\n",
    "<h4>Drexel University - MEM T680 - Fall 2023</h4>\n",
    "<h4>Author: Anupam Mishra</h4>\n",
    "\n",
    "This report outlines my Final Project for MEM T680: Data Analysis and Machine Learning.\n",
    "It discusses the steps taken to develop supervised and unsupervised machine learning models to help determine measures of observability for the atmospheres of confirmed exoplanets throughout the observable universe.\n",
    "\n",
    "**Note:** All of the plots that are generated in this report have been saved under docs/plots in case they need to be accessed.\n",
    "\n",
    "<h3>Background Information</h3>\n",
    "Exoplanets are defined as any planet that orbits a star outside of the Solar System. To date, roughly 7000 exoplanets have been confirmed and many more are being discovered by the day through incredibly powerful space telescope such as the James Webb Space Telescope, TESS (Transiting Exoplanet Survey Satellite), Kepler, and more. The discovery and subsequent study of these exoplanets can teach astronomers much about the universe by allowing them to categorize these planets and learn their characteristics, going so far as to determine whether they may be sustainable to support life. There are countless planets to be discovered and countless observations to be made.\n",
    "\n",
    "Unfortunately, not all exoplanets are equally observable with today's technology. Factors included in the data that will be discussed in this report can greatly impact how well an exoplanet's atmosphere can be observed, with this number varying significantly and in complex ways depending on the combination of parameters that define a given exoplanet. The machine learning models implemented in this project will aim to tackle that problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset Selection</h3>\n",
    "\n",
    "<p>The dataset used in this analysis contains information on confirmed exoplanets with various attributes, spanning across 18,428 different observations. Here are the key columns in the dataset:\n",
    "\n",
    "<strong>\n",
    "<ul>\n",
    "<li>Planet Name (Planet_Name)</li>\n",
    "<li>1.5 micron emission SNR (signal-to-noise ratio) relative to HD 209458 b (SNR_Emission_15_micron)</li>\n",
    "<li>5 micron emission SNR relative to HD 209458 b (SNR_Emission_5_micron)</li>\n",
    "<li>K-band Transmission SNR relative to HD 209458 b (SNR_Transmission_K_mag)</li>\n",
    "<li>Planet radius (Rp) [Jupiter radii]</li>\n",
    "<li>Planet mass (Mp) [Jupiter masses]</li>\n",
    "<li>Dayside temperature (Tday) [K]</li>\n",
    "<li>Planet equilibrium temperature (Teq) [K]</li>\n",
    "<li>Planet surface gravity (log10g_p) [log(cm/s^2)]</li>\n",
    "<li>Planet orbital period (Period) [days]</li>\n",
    "<li>Planet transit duration (Transit_Duration) [hours]</li>\n",
    "<li>K-band magnitude (K_mag) [mag]</li>\n",
    "<li>Distance to planet host star (Distance) [parsecs]</li>\n",
    "<li>Stellar effective temperature (Teff) [K]</li>\n",
    "<li>Stellar surface gravity (log10g_s) [log(cm/s^2)]</li>\n",
    "<li>Planet transit flag (Transit_Flag) - FALSE or TRUE</li>\n",
    "<li>Planet source catalog name (Catalog_Name)</li>\n",
    "</ul>\n",
    "</strong>\n",
    "</p>\n",
    "\n",
    "<h3>Why is it useful?</h3>\n",
    "\n",
    "<p>\n",
    "The dataset is valuable for exoplanet scientists and researchers interested in atmospheric characterization through transmission and emission in the near- and mid-infrared wavebands. Key attributes such as emission SNR, transmission SNR, planet radius, mass, temperatures, and more, provide crucial information for determining the suitability of exoplanets for atmospheric studies.\n",
    "\n",
    "With the upcoming launch of the James Webb Space Telescope (JWST), this dataset, particularly the provided signal-to-noise ratio (SNR) metrics, can aid in selecting optimal targets for atmospheric characterization. The dataset allows users to filter and sort exoplanets based on their expected signal strength, considering various observational parameters.\n",
    "\n",
    "The inclusion of parameters like equilibrium temperature, surface gravity, and transit flags further enhances the dataset's utility in identifying potential candidates for detailed atmospheric studies. Overall, this dataset serves as a valuable resource for astronomers and researchers in the field of exoplanet studies, contributing to the selection of targets for JWST observations and advancing our understanding of exoplanetary atmospheres.\n",
    "</p>\n",
    "\n",
    "<h3>Import Libraries & Load Dataset</h3>\n",
    "<p> First, we import the modules we need and load the dataset from the CSV file:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"search.csv\")  # Load the dataset from search.csv\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding Atmospheric Height</h3>\n",
    "<p>A new column, 'Atmospheric_Height', is added to the dataset. This column represents the atmospheric height and is calculated using an established formula:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the atmospheric height H based on existing formulas\n",
    "\n",
    "df['H'] = 1000*8.3144598*df['Teq']/2.3/df['log10g_p']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initial Data Visualization</h2>\n",
    "\n",
    "To get a sense of what the raw dataset looks like, we can create some visualizations to interpret its nature and characteristics.\n",
    "\n",
    "<h3>Plot 0: Missing Data Heatmap (Before Preprocessing)</h3>\n",
    "\n",
    "<p>This data is vast and encompasses many recently-found exoplanets. For this reason, there is missing data within the dataset that must be accounted for to prevent it from negatively affecting a training model. To identify categories with large amounts of missing data, we can generate a heatmap as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "missing_values = df.isnull()\n",
    "\n",
    "# Develop heatmap to visualize missing values\n",
    "# Plot 0: Heatmap Showing Missing Values in Categories (Unprocessed/Raw Data)\n",
    "heatmap = sns.heatmap(data = missing_values, yticklabels=False, cbar=False, cmap='viridis') # Heatmap format\n",
    "heatmap.set_title('Missing Data Before Preprocessing') # Heatmap title\n",
    "plt.show() # Display heatmap\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3></h3>\n",
    "\n",
    "<h3>Plot 1: Scatter plot of Planet Mass vs. 5 Micron Emission SNR with K mag Transmission SNR Colormap</h3>\n",
    "\n",
    "The below scatterplot plots Planet Mass vs. 5 Micron Emission SNR with K mag Transmission SNR Colormap. The size of each data point represents the exoplanet's 1.5 Micron Emission SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Visualizations\n",
    "\n",
    "# Plot 1: Scatter plot of Planet Mass vs. 5 Micron Emission SNR\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with colormap and size based on Transit Duration\n",
    "plt.scatter(df['Mp'], df['SNR_Emission_5_micron'], c=df['SNR_Transmission_K_mag'], cmap='viridis', s=df['SNR_Emission_15_micron'], alpha=0.8)  \n",
    "plt.xlabel('Planet Mass (Log-scale)')  # X label\n",
    "plt.ylabel('5 Micron Emission SNR (Signal to Noise Ratio, Log-scale)')  # Y label\n",
    "plt.xscale('log')  # Set logarithmic scale for X-axis\n",
    "plt.yscale('log')  # Set logarithmic scale for Y-axis\n",
    "plt.title('Planet Mass vs 5 Micron Emission SNR with K-band Transmission SNR Colormap')  # Plot 1 Title\n",
    "plt.colorbar(label='K-band Transmission SNR (Signal to Noise Ratio)')  # Set colorbar\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shown obvious outliers that must be dealt with during preprocessing. There is a noticeable positive correlation between the Planet Mass and the SNR values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot 2: Scatterplot of Planet Radius vs. Planet Mass with 1.5 Micron Emission SNR Colormap</h3>\n",
    "\n",
    "The below scatterplot plots Planet Radius vs. Planet Mass with 1.5 Micron Emission SNR Colormap. The size of each data point represents the exoplanet's Transit Duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Scatter plot of Planet Radius vs. Planet Mass\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with colormap and size based on Transit Duration\n",
    "plt.scatter(df['Rp'], df['Mp'], c=df['SNR_Emission_15_micron'], cmap='viridis', s=df['Transit_Duration']*10, alpha=0.8)  \n",
    "plt.xlabel('Planet Radius (Log-scale)')  # X label\n",
    "plt.ylabel('Planet Mass, Log-scale)')  # Y label\n",
    "plt.xscale('log')  # Set logarithmic scale for X-axis\n",
    "plt.yscale('log')  # Set logarithmic scale for Y-axis\n",
    "plt.title('Planet Mass vs Planet Radius with 1.5 Micron Emission SNR Colormap')  # Plot 2 Title\n",
    "plt.colorbar(label='1.5 Micron Emission SNR (Signal to Noise Ratio)')  # Set colorbar\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an evident correlation between Planet Radius, Planet Mass, and Transit Duration. However, this plot isn't telling us much about the SNR values in the colormap since it hasn't been normalized and there are some outliers present."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot 3: Pie Chart of Greatest SNR Value</h3>\n",
    "\n",
    "A pie chart plotting the largest SNR value for all of the data points can tell us about which of them has the greatest chance of having a relatively high value. This can provide insights into which of the SNR values should be used for machine learning targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Pie Chart of Greatest SNR Values between the given 3\n",
    "\n",
    "# Identify the column with the largest value for each row\n",
    "df['Largest_SNR'] = df[['SNR_Emission_15_micron', 'SNR_Emission_5_micron', 'SNR_Transmission_K_mag']].idxmax(axis=1)  # Find the column with the largest SNR value for each row\n",
    "\n",
    "# Count the occurrences of each category\n",
    "counts = df['Largest_SNR'].value_counts()  # Count the occurrences of each category\n",
    "\n",
    "# Create a static pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral', 'lightgreen'])  # Pie chart with percentages and colors\n",
    "plt.title('Distribution Based on Largest SNR')  # Pie chart title\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot 4: Pair Plots</h3>\n",
    "\n",
    "We can build pair plots to graph different data features against each other within the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot 4: Pair Plots\n",
    "# Assuming 'df' is your DataFrame with relevant columns\n",
    "# Include the features and target variables in the pair plot\n",
    "columns_to_plot = ['Rp', 'Mp', 'Distance', 'Period', 'log10g_p', 'Teq','SNR_Emission_15_micron', 'SNR_Emission_5_micron', 'SNR_Transmission_K_mag']\n",
    "\n",
    "# Create a subset of the DataFrame with the selected columns\n",
    "df_subset = df[columns_to_plot]\n",
    "\n",
    "# Plotting pair plot\n",
    "sns.pairplot(df_subset, diag_kind='kde', markers='o', palette='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the data is not presented very well. This is due to there having been no preprocessing or normalization done to scale the data and make it easier to understand. That will be the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing of Data</h2>\n",
    "<p>With a large amount missing from a few categories, notable the SNR values, Planet Mass, Planet Gravity, Transit Duration, and Atmospheric Height, we preprocess the data to create a cleaner, usable dataset without discrepancies and bias.</p>\n",
    "\n",
    "<h4>Data Cleaning</h4>\n",
    "<p>The data is cleaned by simply removing observation data with missing values for any given features. Doing this ensures that there are no issues in training a machine learning model since all data being used will have all relevant features included.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing - remove rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Percentile Scaling (Median-MAD Scaling) Normalization</h4>\n",
    "<p>Normalization is performed to the dataset so that it is better suited for machine learning, and isn't affected by large outliers. To effectively scale the data without having significant effect from outliers, we can use Min-Max Scaling. This form of normalization transforms features to a specific range, usually [0,1]. Min-Max Scaling can be described by this formula:\n",
    "<p>\n",
    "<p>X<sub>scaled</sub> = (X - X<sub>min</sub>) / (X<sub>max</sub> - X<sub>min</sub>) </p>\n",
    "\n",
    "\n",
    "\n",
    "<b>Steps to perform Min-Max Scaling</b>\n",
    "<strong>\n",
    "<ol>\n",
    "<li>Define columns of the dataset</li>\n",
    "<li>Apply MinMax Scaling using MinMaxScaler() from Scikit Learn</li>\n",
    "<li>Remove outliers based on scaled values with threshold of 0.95</li>\n",
    "</ol>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MinMaxScaler from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming X is your dataset\n",
    "numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Create an instance of the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMax scaling to the numeric columns in the DataFrame\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Define a threshold for outlier removal (adjust as needed)\n",
    "threshold = 0.95\n",
    "\n",
    "# Create a boolean mask identifying rows with values above the threshold for all numeric columns\n",
    "outlier_mask = (df[numeric_columns] >= threshold).all(axis=1)\n",
    "\n",
    "# Remove rows identified by the outlier mask (keeping rows where at least one value is below the threshold)\n",
    "df = df[~outlier_mask]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Other Types of Normalization (Attempted)</h4> \n",
    "\n",
    "<h5>Z-score Normalization</h5>\n",
    "<p>Since there are some pretty big outliers in the exoplanet data, I had attempted a Z-score normalization. The z-score of a data point measures how many standard deviations away it is away from the mean of the feature. To get rid of major outliers while still preserving most of the data, the Z-score normalization was applied with a threshold value of 3. This meant that data within 3 standard deviations from the mean of each feature was kept, while those data points outside of this threshold were dropped. Unfortunately, too much of the data was rejected and this affected K-means clustering later on, so I decided not to go with it.</p>\n",
    "\n",
    "<h5>Robust Scaling Normalization</h5>\n",
    "<p>I also tried to perform Robust Scaling, based on the median and interquartile range. This uses the median and IQR for scaling rather than the median and standard deviation, making it more robust when dealing with extreme values. It is denoted by the following formula:</p>\n",
    "\n",
    "<p>X<sub>scaled</sub> = (X - Median(X)) / IQR(X) </p>\n",
    "\n",
    "<p>Unfortunately, this normalization method was not suitable for visualizing the results of the machine learning methods implemented, and so it was rejected.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization of Preprocessed Data</h2>\n",
    "\n",
    "<h3>Plot 1: Scatter plot of Planet Mass vs. 5 Micron Emission SNR with K mag Transmission SNR Colormap</h3>\n",
    "<p>To visualize the relationship between the mass of exoplanets, the emission signal-to-noise ratio (SNR) at a frequency of 5 microns, and K-band transmission SNR, a scatterplot with a colormap is plotted. This can provide important information on how a planet's mass affects its likeliness to be a candidate for atmospheric studies based on how strong of a signal it can produce during transit relative to the noise received from its stellar flux and surroundings. The colors of the datapoints represent the valye of the K-band transmission SNR, while the size represents the 1.5 Micron Emission SNR.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Visualizations\n",
    "\n",
    "# Plot 1: Scatter plot of Planet Mass vs. 5 Micron Emission SNR\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with colormap and size based on Transit Duration\n",
    "plt.scatter(df['Mp'], df['SNR_Emission_5_micron'], c=df['SNR_Transmission_K_mag'], cmap='viridis', s=df['SNR_Emission_15_micron']*1000, alpha=0.8)  \n",
    "plt.xlabel('Planet Mass (Log-scale)')  # X label\n",
    "plt.ylabel('5 Micron Emission SNR (Signal to Noise Ratio, Log-scale)')  # Y label\n",
    "plt.xscale('log')  # Set logarithmic scale for X-axis\n",
    "plt.yscale('log')  # Set logarithmic scale for Y-axis\n",
    "plt.title('Planet Mass vs 5 Micron Emission SNR with K-band Transmission SNR Colormap')  # Plot 1 Title\n",
    "plt.colorbar(label='K-band Transmission SNR (Signal to Noise Ratio)')  # Set colorbar\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatterplot is much better than the one generated before preprocessing since major outliers have been removed and the data has been effectively normalized. You can see a correlation between Planet Mass and the 5 Micron and K-band SNR values in this plot. The colormap is far more distinct than it was before preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot 2: Scatterplot of Planet Radius vs. Planet Mass with 1.5 Micron Emission SNR Colormap</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Scatter plot of Planet Radius vs. Planet Mass\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with colormap and size based on Transit Duration\n",
    "plt.scatter(df['Rp'], df['Mp'], c=df['SNR_Emission_15_micron'], cmap='viridis', s=df['Transit_Duration']*1000, alpha=0.8)  \n",
    "plt.xlabel('Planet Radius (Log-scale)')  # X label\n",
    "plt.ylabel('Planet Mass, Log-scale)')  # Y label\n",
    "plt.xscale('log')  # Set logarithmic scale for X-axis\n",
    "plt.yscale('log')  # Set logarithmic scale for Y-axis\n",
    "plt.title('Planet Mass vs Planet Radius with 1.5 Micron Emission SNR Colormap')  # Plot 1 Title\n",
    "plt.colorbar(label='1.5 Micron Emission SNR (Signal to Noise Ratio)')  # Set colorbar\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot better than the previous plot (before preprocessing). The colormap is now visible and there is a clear correlation between the variables, absent of any major outliers. There is also a positive correlation between SNR and Transit Duration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot 3: Pie Chart of Greatest SNR Value</h3>\n",
    "<p>A pie chart is plotted again to identify which out of the three SNR values (1.5 micron Emission, 5 micron Emission, and K-band Transmission) is greatest for each exoplanet after preprocessing. This can provide insight into which categories are most likely to produce favorable results in identifying exoplanets with observable atmospheres.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Pie Chart of Greatest SNR Values between the given 3\n",
    "\n",
    "# Identify the column with the largest value for each row\n",
    "df['Largest_SNR'] = df[['SNR_Emission_15_micron', 'SNR_Emission_5_micron', 'SNR_Transmission_K_mag']].idxmax(axis=1)  # Find the column with the largest SNR value for each row\n",
    "\n",
    "# Count the occurrences of each category\n",
    "counts = df['Largest_SNR'].value_counts()  # Count the occurrences of each category\n",
    "\n",
    "# Create a static pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral', 'lightgreen'])  # Pie chart with percentages and colors\n",
    "plt.title('Distribution Based on Largest SNR')  # Pie chart title\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pie chart shows that the proportion of data values in which SNR 15 is the greatest of the three signal-to-noise ratios has gone down dramatically after preprocessing, with Kband Transmission SNR being the most commonly greatest for the vast majority of data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot 4: Pair Plots</h3>\n",
    "\n",
    "<p>Pair plots are generated again to visualize the interrelatedness of the features within the dataset after preprocessing and understand what trends and patterns a machine learning model might pick up on amongst them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot 4: Pair Plots\n",
    "# Assuming 'df' is your DataFrame with relevant columns\n",
    "# Include the features and target variables in the pair plot\n",
    "columns_to_plot = ['Rp', 'Mp', 'Distance', 'Period', 'log10g_p', 'Teq','SNR_Emission_15_micron', 'SNR_Emission_5_micron', 'SNR_Transmission_K_mag']\n",
    "\n",
    "# Create a subset of the DataFrame with the selected columns\n",
    "df_subset = df[columns_to_plot]\n",
    "\n",
    "# Plotting pair plot\n",
    "sns.pairplot(df_subset, diag_kind='kde', markers='o', palette='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pair plots are much cleaner now after normalization! There is a clear non-linear relationship betweeen some of the features, particularly Planet Radius (Rp), Distance, and the SNR values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interactive Scatterplot</h2>\n",
    "\n",
    "<p>An interactive scatterplot is created using Dash to provide a user interface to change the plot. The x and y axes are fixed as the planet radius and planet mass, respectfully, while the colormap variable can be selected from a list of all available categories within the dataset. The size of the points represents 'Transit Duration' for each exoplanet. This scatterplot can be found in src/exoplanets/hw4-plots.py</p>\n",
    "\n",
    "<h2>Machine Learning Challenge Description</h2>\n",
    "\n",
    "The goal of this project is <b>to train machine learning models to determine and rank the relative observability of an exoplanet’s atmosphere for future studies based on its measured properties.</b> In the context of future astronomical studies, the focus is on understanding and predicting the degree to which an exoplanet's atmosphere can be observed. This predictive capability is derived from leveraging the measured properties of exoplanets, encompassing fundamental features such as size, mass, surface temperature, and other key characteristics.\n",
    "\n",
    "Essentially, this challenge seeks to answer questions such as: How do the size, mass, and temperature of an exoplanet influence the observability of its atmosphere? Can we identify specific features that are more indicative of observable atmospheres? Through the lens of machine learning, the goal is to distill meaningful information from the data, enabling the creation of models that can not only predict observability but also rank exoplanets in terms of their atmospheric visibility for prospective studies. This kind of study can work towards laying the groundwork for future endeavors in the exploration and studying of exoplanetary atmospheres.\n",
    "\n",
    "This machine learning problem is challenging due to the multidimensional nature of the dataset, the need for effective feature selection and scaling, potential class imbalances, and the application of both supervised and unsupervised learning techniques.\n",
    "<ul>\n",
    "<li>Data Complexity: The features I use for predictions, such as exoplanet radius, mass, gravity, and distance all contribute to the data's complexity. Some features have nonlinear relationships with the Emission SNR values, which can pose challenges in uncovering trends within the data to train a model.</li>\n",
    "<li>Feature Selection: The features I select to train the machine learning models can heavily impact the model's performance. Scaling and normalization can help to tackle this, but features and their interrelationships must be closely analyzed before a model is trained.</li>\n",
    "\n",
    "<li>The Curse of Dimensionality: Dimensionality can pose issues by making data sparse and difficult to use for training purposes. Since there are several features in my data, dimensionality may create problems in its implementation within machie learning models. Dimensionality reduction methods such as Principal Component Analysis (PCA) will need to be explored for this reason.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h2>Supervised Learning</h2>\n",
    "Supervised learning methods were employed to train a model to map from various exoplanet parameters presented in the dataset to SNR values so that it can predict these values for new, unseen data and draw conclusions on the atmosphere observability of new exoplanets.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest Regression</h3>\n",
    "\n",
    "\n",
    "Random Forest Regression, often referred to as Random Forest Regressor, is a machine learning algorithm that belongs to the ensemble learning family. Specifically, it is an extension of the Random Forest algorithm applied to regression problems. Random Forest Regression is used for predicting a continuous variable (regression), as opposed to classification problems where the goal is to predict a categorical label. This is the first supervised learning method I attempted due to its ability to handle outliers and noise in a dataset and perform highly accurate predictions for non-linear data.\n",
    "\n",
    "In implementing Random Forest, a pipeline is established which takes the preprocessed data and\n",
    "\n",
    "1. Performs a Test-Train Split\n",
    "2. Standardizes the features using StandardScaler()\n",
    "3. Performs Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "Hyperparameters considered (& best parameters):\n",
    "- `n_estimators`: number of trees in the forest (100)\n",
    "- `max_depth`: maximum depth of the tree (None)\n",
    "- `min_samples_split`: minimum number of samples needed to split an internal node (5)\n",
    "- `min_samples_leaf`: minimum number of samples needed to be at a leaf node (2)\n",
    "    \n",
    "4. Evaluates & Visualizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features (X) and Target (y) for SNR prediction\n",
    "X_regression = df[['Rp', 'Mp', 'Distance', 'log10g_p']]  # Add other features as needed\n",
    "y_regression = df[['SNR_Emission_15_micron', 'SNR_Emission_5_micron']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_regression, X_test_regression, y_train_regression, y_test_regression = train_test_split(\n",
    "    X_regression, y_regression, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_regression_scaled = scaler.fit_transform(X_train_regression)\n",
    "X_test_regression_scaled = scaler.transform(X_test_regression)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model with the best parameters\n",
    "grid_search.fit(X_train_regression_scaled, y_train_regression)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Predict SNR values on the test set using the best model\n",
    "best_regressor = grid_search.best_estimator_\n",
    "y_pred_regression = best_regressor.predict(X_test_regression_scaled)\n",
    "\n",
    "# Evaluate the regression model\n",
    "mse_regression = mean_squared_error(y_test_regression, y_pred_regression)\n",
    "print(f'Mean Squared Error (Regression): {mse_regression}')\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = best_regressor.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_regression.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
    "plt.title('Feature Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "# Predict SNR values for the entire dataset\n",
    "df['Predicted_SNR_Emission_15_micron'], df['Predicted_SNR_Emission_5_micron'] = best_regressor.predict(X_regression).T\n",
    "\n",
    "# Plot training and predicted SNR 15 values against Rp\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Training Data - SNR 15 vs. Rp\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=df, x='Rp', y='SNR_Emission_15_micron', palette='viridis')\n",
    "plt.yscale('log')\n",
    "plt.title('Training Data - SNR 15 vs. Rp')\n",
    "\n",
    "# Predicted Data - SNR 15 vs. Rp\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=X_test_regression['Rp'], y=df['Predicted_SNR_Emission_15_micron'], palette='viridis')\n",
    "plt.yscale('log')\n",
    "plt.title('Predicted Data - SNR 15 vs. Rp')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results of Random Forest Regression</h4>\n",
    "The Mean-Squared Error for Random Forest is 0.00034446655413966214.\n",
    "Therefore, this method does a good job of mapping SNR values from the given features.\n",
    "\n",
    "The Feature Importance Scores plot above reflects the importance of the included features within the Random Forest Regression. Planet Radius and Distance seem to be the most important to the regression model, with Planet Mass and Planet Gravity being less important. This reflects how the dimensionality of the model could be reduced, by excluding the less important features.\n",
    "\n",
    "The second plot compares the Training Data and Predicted Data from the Random Forest model. There is an evident similarity between both plots, as both follow positive trends. The model might however be facing an issue of overfitting and might not provide as accurate mapping when provided other unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient-Boosting Regression</h3>\n",
    "Gradient Boosting Regression is a supervised machine learning ensemble method tailored for regression tasks, predicting continuous target variables. This approach constructs a sequence of decision trees sequentially, each tree refining the errors made by the combined ensemble of the preceding trees.\n",
    "\n",
    "Implementation of Gradient-Boosting involves the establishment of a pipeline that takes preprocessed data and performs the following steps:\n",
    "\n",
    "1. **Test-Train Split:** The dataset is split into training and testing sets to facilitate model evaluation.\n",
    "   \n",
    "2. **Initialization of GradientBoostingRegressor():** The Gradient Boosting Regressor model is initialized with default hyperparameters.\n",
    "   \n",
    "3. **Model Evaluation:** The initial model is evaluated on both the training and testing sets using the Mean Absolute Error (MAE) metric.\n",
    "\n",
    "4. **Hyperparameter Tuning using GridSearchCV:** A GridSearchCV is employed to systematically explore hyperparameter combinations. The hyperparameters considered are:\n",
    "   - `max_depth`: maximum depth of individual regression estimators.\n",
    "   - `alpha`: alpha parameter of the Huber loss function, defining the threshold for outliers.\n",
    "   - `learning_rate`: a regularization parameter controlling the step size in the update process.\n",
    "\n",
    "   The best hyperparameters obtained from the grid search are used to initialize the Gradient Boosting Regressor.\n",
    "\n",
    "5. **Cross-Validated Mean MAE:** Cross-validation is incorporated into the model evaluation process to provide a more robust assessment of its performance. The Cross-Validated Mean MAE is calculated, taking into account the average MAE across different folds. It comes out to Cross-validated Mean MAE: 0.0019066009974601944. This is a very low number, suggesting that the model has good accuracy.\n",
    "\n",
    "6. **Evaluation & Visualization of the Best Model:** The final model, configured with the optimal hyperparameters, is evaluated on the testing set. Additionally, visualizations are generated to illustrate the performance of the model.\n",
    "\n",
    "The Mean Absolute Error for Gradient-Boosting on both the training and testing sets being 0 raises concerns about overfitting. This suggests that the model may be too accurate to the training data, resulting in exceptional predictive accuracy but potentially limited generalizability to unseen data. The incorporation of cross-validation provides a more nuanced understanding of the model's performance, indicating areas for improvement and ensuring a more robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for model evaluation\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    return mae_train, mae_test, y_test_pred\n",
    "\n",
    "# Function for hyperparameter tuning\n",
    "def tune_hyperparameters(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    return best_params\n",
    "\n",
    "# Function for cross-validated evaluation\n",
    "def cross_validate_model(model, X, y):\n",
    "    mae_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return -mae_scores  # Negate the scores to obtain positive MAE values\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_results(actual, predicted, feature_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Actual SNR 15 vs. Feature\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(data=df, x=feature_name, y=actual, palette='viridis')\n",
    "    plt.yscale('log')\n",
    "    plt.title(f'Actual SNR 15 vs. {feature_name}')\n",
    "\n",
    "    # Predicted SNR 15 vs. Feature\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=X_test[feature_name], y=predicted, palette='viridis')\n",
    "    plt.yscale('log')\n",
    "    plt.title(f'Predicted SNR 15 vs. {feature_name}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1a. Supervised Learning (Gradient Boosting Regression)\n",
    "# Define features (X) and target variable (y)\n",
    "features = ['Rp', 'Mp', 'Tday', 'Teq', 'log10g_p', 'Period', 'Transit_Duration', 'K_mag', 'Distance', 'Teff', 'log10g_s']\n",
    "target = 'SNR_Emission_15_micron'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=200, random_state=0)\n",
    "\n",
    "# Hyperparameter tuning for regularization\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 0.9],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_params = tune_hyperparameters(GradientBoostingRegressor(n_estimators=200, random_state=0), param_grid, X_train, y_train)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Fit the model with the best parameters\n",
    "best_gb_regressor = GradientBoostingRegressor(n_estimators=200, random_state=0, **best_params)\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "y_train_pred_best = best_gb_regressor.predict(X_train)\n",
    "y_test_pred_best = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Cross-validated evaluation\n",
    "mae_scores_cv = cross_validate_model(best_gb_regressor, X, y)\n",
    "\n",
    "# Print mean MAE scores across folds\n",
    "print(\"Cross-validated Mean MAE:\", mae_scores_cv.mean())\n",
    "\n",
    "# Print MAE for training and testing sets\n",
    "mae_train, mae_test, _ = evaluate_model(best_gb_regressor, X_train, X_test, y_train, y_test)\n",
    "print(f\"MAE on Training Set: {mae_train:.2f}\")\n",
    "print(f\"MAE on Testing Set: {mae_test:.2f}\")\n",
    "\n",
    "# Scatter plot of predicted SNR values vs. actual SNR values\n",
    "plot_results(y_test, y_test_pred_best, 'Rp')\n",
    "plot_results(y_test, y_test_pred_best, 'Teq')\n",
    "plot_results(y_test, y_test_pred_best, 'log10g_p')\n",
    "plot_results(y_test, y_test_pred_best, 'Distance')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results of Gradient-Boosting Regression</h4>\n",
    "\n",
    "The Mean Absolute Error for Gradient-Boosting on both the training and testing sets being 0 raises concerns about overfitting. This suggests that the model may be too accurate to the training data, resulting in exceptional predictive accuracy but potentially limited generalizability to unseen data. The incorporation of cross-validation provides a more nuanced understanding of the model's performance, indicating areas for improvement and ensuring a more robust evaluation.\n",
    "\n",
    "The Cross-Validated Mean MAE is also calculated, taking into account the average MAE across different folds. It comes out to Cross-validated Mean MAE: 0.0019066009974601944. This is a very low number, suggesting that the model has good accuracy.\n",
    "\n",
    "The above plots all depict the actual test values vs the predicted values estimated by the Gradient-Boosting Regressor for SNR vs. Planet Radius, Equilibrium Temperature, Planet Gravity, and Planet-Star Distance. All plots reflect similarities between correlations and trends within actual and predicted models, suggesting that the Gradient-Boosting Regressor is quite accurate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unsupervised Learning</h3>\n",
    "Unsupervised models can be applied here to cluster data, perform dimensionality reduction, and better understand the local relationships between data points.\n",
    "\n",
    "<h3>K-Means Clustering</h2>\n",
    "\n",
    "K-Means Clustering is an unsupervised machine method used to cluster data into distinct groups (clusters) based on similarity. The goal of K-Means is to partition a dataset into K clusters, where each data point belongs to the cluster with the nearest mean (centroid).\n",
    "\n",
    "Although the machine learning problem I'm looking to solve deals more with unsupervised learning, we can still apply clustering to see how exoplanets might be categorized based on their levels of observability.\n",
    "\n",
    "In implementing K-means, a pipeline is established which takes the preprocessed data and\n",
    "\n",
    "1. Feature Selection: to analyze the relationship between 1.5 Micron Emission SNR, 5 Micron Emission SNR, and Planet Radius, these 3 features are chosen. Adding more features would result in a lower silhouette score.\n",
    "2. Imputes the data using a SimpleImputer()\n",
    "3. Scales the data using a StandardScaler()\n",
    "4. Applies the K-Means model\n",
    "5. Performs Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "Hyperparameters considered (& Best Parameters):\n",
    "- `n_clusters`: number of clusters to form (# of centroids to generate)\n",
    "- `init`: method for initializing centroids (k-means++)\n",
    "- `max_iter`: maximum # of iterations for the KMeans algorithm for a single run (200)\n",
    "- `n_init`: number of times KMeans algorithm will be run with different centroid seeds.\n",
    "    \n",
    "6. Evaluates & Visualizes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Unsupervised Learning (K-means Clustering)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Features for clustering\n",
    "X_cluster = df[['SNR_Emission_15_micron', 'SNR_Emission_5_micron', 'Rp']]\n",
    "\n",
    "# Apply log transformation to the features\n",
    "X_cluster_log = np.log1p(X_cluster)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "X_cluster_log_imputed = imputer.fit_transform(X_cluster_log)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_cluster_scaled = scaler.fit_transform(X_cluster_log_imputed)\n",
    "\n",
    "# KMeans pipeline\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('kmeans', KMeans())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for KMeans\n",
    "param_grid = {\n",
    "    'kmeans__n_clusters': [1,2,3,4],\n",
    "    'kmeans__init': ['k-means++', 'random'],\n",
    "    'kmeans__max_iter': [100, 200, 300],\n",
    "    'kmeans__n_init': [10]\n",
    "}\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(kmeans_pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_cluster_scaled)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Fit the pipeline with the best parameters\n",
    "best_kmeans_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Predict clusters\n",
    "df['Cluster'] = best_kmeans_pipeline.named_steps['kmeans'].predict(X_cluster_scaled)\n",
    "\n",
    "# Explore the resulting clusters\n",
    "for cluster_label in range(best_params['kmeans__n_clusters']):\n",
    "    print(f'\\nCluster {cluster_label}:')\n",
    "    print(df[df['Cluster'] == cluster_label])\n",
    "\n",
    "# Evaluate the silhouette score\n",
    "silhouette_score_kmeans = silhouette_score(X_cluster_scaled, df['Cluster'])\n",
    "print(f\"Silhouette Score on KMeans Clusters: {silhouette_score_kmeans}\")\n",
    "\n",
    "# Scatter plot of clustered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for cluster_label in range(best_params['kmeans__n_clusters']):\n",
    "    cluster_data = X_cluster_scaled[df['Cluster'] == cluster_label]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 2], label=f'Cluster {cluster_label}')\n",
    "\n",
    "# Plot centroids\n",
    "centroids = best_kmeans_pipeline.named_steps['kmeans'].cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 2], marker='X', s=200, c='red', label='Centroids')\n",
    "\n",
    "plt.title('K-means Clustering Visualization')\n",
    "plt.xlabel('SNR_Emission_15_micron (scaled)')\n",
    "plt.ylabel('Planet Radius')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of SNR_Emission_15_micron vs. SNR_Emission_5_micron with color-coded KMeans clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(data=df, x='Teq', y='SNR_Emission_15_micron', hue='Cluster', palette='viridis')\n",
    "plt.yscale('log')\n",
    "plt.title('SNR_Emission_15_micron vs. Teq with KMeans Clusters (log scale)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=df, x='Rp', y='SNR_Emission_15_micron', hue='Cluster', palette='viridis')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title('SNR_Emission_15_micron vs. Rp with KMeans Clusters (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results of K-Means Clustering</h4>\n",
    "\n",
    "Accuracy:\n",
    "The Silhouette Score on the Kmeans clusters is 0.79108227983781. This score measures how well defined each cluster is and accounts for things such as overlap. The received score suggests that data points are mostly well-matched to their own respective clusters and not matched as well to neighboring clusters, but there are exceptions. Soem overlap between clusters is still likely and so this method doesn't perform best.\n",
    "\n",
    "This suggests that the relationship between SNR values and Planet Radius (Rp) can help efficiently categorize exoplanets based on atmosphere observability if so desired.\n",
    "\n",
    "However, adding more features to the K-Means algorithm such as Distance and Equilibrium Temperature (not implemented in this report) reduced the silhouette score significantly and resulted in much more overlap. This is due to high dimensionality in the model.\n",
    "\n",
    "Plot Results:\n",
    "The first plot above shows the K=means clustering between 1.5 Micron SNR Emission and Planet Radius.\n",
    "This plot shows some evident overlap between clusters, and overall poor performance. The 'X' points denote the cluster centroids, which are far apart and reflect good clustering.\n",
    "\n",
    "The second set of plots above show the K-means clusters as hues for plots for 15 Micron SNR Emission vs. Equilibrium Teperature and vs. Planet Radius. Although there is decent clustering,the plots show that there is some overlap between clusters, so this method is not perfect. Clusters are very close together and there isn't a strict distinction between them. Kmeans clustering may not be the best method for the purposes of this dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Principal Component Analysis</h3>\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique for machine learning. It's primary goal is to transform the original features of a dataset into a new set of uncorrelated features (principal components), which capture the maximum variance in the data.\n",
    "\n",
    "We can apply PCA to our data to perform dimensionality reduction and to visualize the high-dimensional data as it is projected onto a lower-dimensional space.\n",
    "\n",
    "In implementing PCA, a pipeline is established which takes the preprocessed data and\n",
    "\n",
    "1. Imputes the data using a SimpleImputer()\n",
    "2. Scales the data using a StandardScaler()\n",
    "3. Applies PCA()\n",
    "4. Performs Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "Hyperparameters considered (& Best Parameters):\n",
    "- `n_components`: number of principal components to retain during PCA (3)\n",
    "- `svd_solver': algorithms for singular value decomposition (SVD) (randomized)\n",
    "- `iterated_power`: specified number of iterations for the power method used in randomized SVD solver\n",
    "\n",
    "    \n",
    "5. Evaluates & Visualizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_cluster_log_imputed is defined\n",
    "\n",
    "# PCA pipeline\n",
    "pca_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for PCA\n",
    "param_grid_pca = {\n",
    "    'pca__n_components': [2, 3, 4],\n",
    "    'pca__svd_solver': ['auto', 'full', 'arpack', 'randomized'],  # Add solver options\n",
    "    'pca__iterated_power': [5, 10, 15]  # Adjust iterated_power\n",
    "}\n",
    "\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search_pca = GridSearchCV(pca_pipeline, param_grid_pca, cv=5)\n",
    "grid_search_pca.fit(X_cluster_log_imputed)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_pca = grid_search_pca.best_params_\n",
    "print(\"Best Parameters for PCA:\", best_params_pca)\n",
    "\n",
    "# Fit and transform the data using PCA with the best parameters\n",
    "X_pca = grid_search_pca.best_estimator_.transform(X_cluster_log_imputed)\n",
    "\n",
    "# Access the principal components\n",
    "principal_components = grid_search_pca.best_estimator_['pca'].components_\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "components_df = pd.DataFrame(principal_components.T, columns=[f'PC{i+1}' for i in range(principal_components.shape[0])])\n",
    "print(\"Principal Components:\")\n",
    "print(components_df)\n",
    "\n",
    "# Visualize the explained variance ratio\n",
    "explained_variance = grid_search_pca.best_estimator_['pca'].explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "# Add the 'Cluster' column to the PCA DataFrame\n",
    "pca_df['Cluster'] = df['Cluster']\n",
    "\n",
    "# Scatter plot of PCA components\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['Cluster'], palette='viridis')\n",
    "plt.title('PCA Components with KMeans Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results of Principal Component Analysis (PCA)</h4>\n",
    "\n",
    "The Explained Variance Ratio is [0.79860487 0.16371345 0.03768169]. This ratios show how much of the variance is accounted for in each principal component, of which there are 3. Since PCA always generates a vector in the direction of maximum variance and projects data onto that axis, the first principal component has the highest variance ratio, with the second having the second most and the third one having the least. \n",
    "\n",
    "This basically helps us with reducing dimensionality. The PCA model has done a good job of doing so.\n",
    "\n",
    "75 of the 180 fits fail during the hyperparameter tuning. More fine tuning may be required to deal with this issue.\n",
    "\n",
    "Plot Results: These results can also be depicted by the PCA plot below, which has a well-defined first principal axis with a shorter second principal axis.. most of the data lies on or close to a single axis (principal axis with maximum variance), with some data branching off onto a second axis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-Means Clustering + PCA</h3>\n",
    "\n",
    "Since PCA offers the ability to reduce dimensionality, I wanted to apply it to the dataset before the K-means clustering to see how it affected the clustering performance. I built the PCA and K-means pipelines and followed the same procedures as above before plotting the results. Unfortunately, the dimensionality reduction led to the data ending up in a single cluster. This is therefore not a good method for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Features for clustering\n",
    "X_cluster = df[['SNR_Emission_15_micron', 'SNR_Emission_5_micron', 'Rp']]\n",
    "\n",
    "# Apply log transformation to the features\n",
    "X_cluster_log = np.log1p(X_cluster)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "X_cluster_log_imputed = imputer.fit_transform(X_cluster_log)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_cluster_scaled = scaler.fit_transform(X_cluster_log_imputed)\n",
    "\n",
    "# PCA pipeline\n",
    "pca_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2))  # Adjust the number of components as needed\n",
    "])\n",
    "\n",
    "# Fit and transform the data using PCA\n",
    "X_pca = pca_pipeline.fit_transform(X_cluster_scaled)\n",
    "\n",
    "# KMeans pipeline\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=3)),  # Align the number of components with PCA\n",
    "    ('kmeans', KMeans())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for KMeans\n",
    "param_grid = {\n",
    "    'kmeans__n_clusters': [1, 2, 3, 4],\n",
    "    'kmeans__init': ['k-means++', 'random'],\n",
    "    'kmeans__max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(kmeans_pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_cluster_log_imputed)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Fit the pipeline with the best parameters\n",
    "best_kmeans_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Predict clusters\n",
    "df['Cluster'] = best_kmeans_pipeline.named_steps['kmeans'].predict(X_cluster_log_imputed)\n",
    "\n",
    "# Scatter plot of PCA components with color-coded KMeans clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['Cluster'], palette='viridis')\n",
    "plt.title('PCA Components with KMeans Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data=df, x='Rp', y='SNR_Emission_15_micron', hue='Cluster', palette='viridis')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title('SNR_Emission_15_micron vs. SNR_Emission_5_micron with KMeans Clusters (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results of K-Means Clustering & PCA</h4>\n",
    "\n",
    "The PCA dimensionality reduction ultimately is putting all the data under a single cluster, as shown in the plots above. This method therefore doesn't work, and so K-means clustering on its own is better for this dataset. The dimensionality reduction performed by the PCA messes with the applicability of K-means clustering on the dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Manifold Learning - t-Stochastic Nearest Neighbor Embeddings (t-SNE)</h3>\n",
    "\n",
    "t-SNE is another dimensionality reduction technique for visualizing high-dimensional data in a lower-dimensional space. It's good to use for non-linear relationships within the data, so it is suitable for the exoplanet dataset. As a manifold learning tachnique, t-SNE can preserve the local relationships between data points while reducing the overall dimensionality of the data.\n",
    "\n",
    "For t-SNE, a pipeline is established which takes the preprocessed data and\n",
    "\n",
    "1. Imputes the data using a SimpleImputer()\n",
    "2. Scales the data using a StandardScaler()\n",
    "3. Applies TSNE()\n",
    "4. Performs Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "Hyperparameters considered (& Best Parameters):\n",
    "- `n_components`: number of components in the low-dimensional representation (2)\n",
    "- `perplexity`: balances attention between local and global aspects of data (30)\n",
    "- `learning_rate`: controls the step size in the optimization process and can impact the convergence and quality of the low-dimensional representation (10)\n",
    "\n",
    "    \n",
    "5. Evaluates & Visualizes\n",
    "\n",
    "The t-SNE method is visualized in the plot below. It offers a visualization of the relationships between the data, and the hue is based on the clusters from the K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifold Learning\n",
    "\n",
    "# Import t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_cluster_log_imputed is defined\n",
    "\n",
    "# t-SNE pipeline\n",
    "tsne_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('tsne', TSNE())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for t-SNE\n",
    "param_grid_tsne = {\n",
    "    'tsne__n_components': [2],  # Adjust the number of components as needed\n",
    "    'tsne__perplexity': [30, 50, 100],\n",
    "    'tsne__learning_rate': [10, 50, 100]\n",
    "}\n",
    "\n",
    "# Specify scoring metric\n",
    "scoring_metric = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search_tsne = GridSearchCV(tsne_pipeline, param_grid_tsne, cv=5, scoring=scoring_metric)\n",
    "grid_search_tsne.fit(X_cluster_log_imputed)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_tsne = grid_search_tsne.best_params_\n",
    "print(\"Best Parameters for t-SNE:\", best_params_tsne)\n",
    "\n",
    "# Fit and transform the data using t-SNE with the best parameters\n",
    "X_tsne = grid_search_tsne.best_estimator_['tsne'].fit_transform(X_cluster_log_imputed)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=df['Cluster'], palette='viridis')\n",
    "plt.title('t-SNE Visualization with Best Parameters')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results of t-SNE</h4>\n",
    "\n",
    "The plot above shows the local relationships within the data and provides insights into the data's structure. Data from Cluster 0 is spread out, but certain data are grouped together suggesting similarities in the high-dimensional space between them and the relationships between associated features. This may be interpreted as being reflective of the results from the PCA method, which showed that data points that fell into cluster 0 demonstrated high variance along one of the principal axes. The other clusters in the t-SNE plot are all closely grouped together. \n",
    "\n",
    "The t-SNE plot successfully described local relationships within the data while reducing dimensionality. However, since a lot of the data is scattered and not grouped together, it suggests that the method has flaws and changes such as fine-tuning could be made to improve it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusions</h2>\n",
    "\n",
    "The analysis and modeling performed as part of this project reflects the importance and applicability of the exoplanet dataset used and the role that machine learning methods can play in making important predictions that can further the advent of space exploration. \n",
    "\n",
    "<h3>Accomplished Goals</h3>\n",
    "Some of the goals that were accomplished in the project include:\n",
    "\n",
    "1. Train supervised learning models to effectively map from features like Planet Radius to the Signal-to-Noise ratios (SNRs) with high accuracy.\n",
    "2. Using unsupervised learning models to perform dimensionality reduction and clustering to better understand the nature and interrelationships within the dataset.\n",
    "3. Visualizing the preprocessed data using multiple plots that describe the relationship between various features.\n",
    "\n",
    "<h3>Future Work & Ways to Improve the Models</h3>\n",
    "There are definite improvements that could be made to the machine learning models applied to the dataset. Some of these include:\n",
    "\n",
    "1. **Exploring Feature Engineering:**\n",
    "   - **Polynomial Features:** Investigating the possibility of introducing polynomial features, especially for Random Forest and Gradient Boosting Regression. Polynomial features can capture complex, non-linear relationships within the data.\n",
    "   - **Feature Interaction:** Experimenting with creating new features that represent interactions between existing features. This approach might uncover valuable insights and enhance the models' ability to capture intricate patterns.\n",
    "   - **Dimensionality Reduction:** Exploring feature reduction techniques, such as Recursive Feature Elimination (RFE) or feature importance analysis. This can help identify and retain the most informative features, streamlining model training.\n",
    "\n",
    "2. **Considering Algorithmic Enhancements:**\n",
    "   - **Ensemble Methods:** Exploring the potential benefits of using different ensemble methods for Random Forest and Gradient Boosting Regression. Stacking or blending multiple models could lead to synergies and improved overall performance.\n",
    "   - **Fine-tuning Hyperparameters:** Conducting a more exhaustive search for hyperparameters, including the number of trees, learning rate, and other relevant parameters. Fine-tuning these values might unlock additional performance gains.\n",
    "   - **Exploring Alternative Libraries:** Experimenting with alternative gradient boosting libraries like XGBoost or LightGBM. These libraries often offer optimizations that can result in faster training and enhanced model performance.\n",
    "\n",
    "3. **Applying Data Augmentation Techniques for Clustering and Dimensionality Reduction:**\n",
    "   - **t-SNE Parameter Adjustment:** In future analyses, considering adjusting the perplexity and learning rate parameters for t-SNE. Fine-tuning these parameters can significantly influence the quality of visualizations and the effectiveness of clustering.\n",
    "   - **Optimizing K Means Initialization:** Experimenting with different initialization methods for K Means clustering. Finding a more suitable initialization strategy may enhance convergence and improve the quality of identified clusters.\n",
    "   - **Enhancing Preprocessing:** Prioritizing effective data preprocessing before applying dimensionality reduction techniques. Ensure robust handling of missing values, and explore scaling or normalizing features to optimize the performance of clustering algorithms. (ChatGPT, 2023)\n",
    "\n",
    "It will be crucial to systematically evaluate the impact of each strategy using appropriate evaluation metrics and cross-validation. With the aforementioned improvement methods, the learning models may have improved performance with more applicable outputs for the goals of this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoplanets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
